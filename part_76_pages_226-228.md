# Embedding Artificial Intelligence into ERP Software A Conceptual View on Business AI with Examples from SAP S4HANA (Siar Sarferaz) (z-library.sk, 1lib.sk, z-lib.sk) - ç¬¬76éƒ¨åˆ†

**åŸå§‹é¡µç **: 226 - 228
**æ®µè½æ•°é‡**: 22
**ç¿»è¯‘å·¥å…·**: Claude Code CLI

## ğŸ“– ç¿»è¯‘ç­–ç•¥å’Œä¸Šä¸‹æ–‡ä¿¡æ¯

- **ä½¿ç”¨çš„ç¿»è¯‘ç­–ç•¥**: å®Œæ•´ä¸Šä¸‹æ–‡
- **å‰æ–‡ä¸Šä¸‹æ–‡é¡µç **: 225-225 (46 å­—ç¬¦)
- **åæ–‡ä¸Šä¸‹æ–‡é¡µç **: 229-229 (1110 å­—ç¬¦)

*æ³¨ï¼šç¿»è¯‘æ—¶å·²ä½¿ç”¨"å®Œæ•´ä¸Šä¸‹æ–‡"ç­–ç•¥ï¼Œå‚è€ƒå‰åæ–‡ä¸Šä¸‹æ–‡ä»¥ç¡®ä¿æœ¯è¯­ä¸€è‡´æ€§å’Œå†…å®¹è¿è´¯æ€§*

**æœ¯è¯­è¯´æ˜**: 1. Model Degradation (æ¨¡å‹è¡°é€€): æŒ‡æ¨¡å‹æ€§èƒ½éšæ—¶é—´æ¨ç§»å› æ•°æ®åˆ†å¸ƒå˜åŒ–è€Œä¸‹é™çš„ç°è±¡ã€‚
2. KPI (å…³é”®ç»©æ•ˆæŒ‡æ ‡): ä¿æŒé€šç”¨ç¼©å†™ã€‚
3. Remodeling (æ¨¡å‹é‡æ„): æ­¤å¤„æŒ‡é‡æ–°è®¾è®¡æˆ–æ„å»ºæ¨¡å‹ç»“æ„ï¼ŒåŒºåˆ«äºå•çº¯çš„Retrainingï¼ˆé‡æ–°è®­ç»ƒå‚æ•°ï¼‰ã€‚
4. Neuronal networks: åŸæ–‡æ‹¼å†™è¾ƒä¸ºå°‘è§ï¼Œæ ‡å‡†æœ¯è¯­ä¸ºNeural Networksï¼Œè¯‘ä¸º'ç¥ç»ç½‘ç»œ'ã€‚
5. Explainability (å¯è§£é‡Šæ€§): AIé¢†åŸŸæ ¸å¿ƒæ¦‚å¿µï¼ŒæŒ‡AIå†³ç­–è¿‡ç¨‹èƒ½è¢«äººç±»ç†è§£çš„ç¨‹åº¦ã€‚

---

These core data model views are typically time-dependent, containing fields for selecting data records for a specific time frame. To evaluate the performance of the artificial intelligence model, the degradation component utilizes the view for model training to access new data that has become available since the last training session. Since implicit feedback is incorporated into the application data, this control dataset includes the actual values for the target variables and the value of the input parameters. The current version of the trained model is used to compute the predictions based on the input values. This is the responsibility of the metrics component, which also compares the actual and predicted values to compute static key performance indicators (KPIs). The degradation component interprets these KPIs and suggests corresponding actions (for instance, retraining or remodeling), which are presented in the monitoring view.

**ã€è¯‘æ–‡ã€‘** è¿™äº›æ ¸å¿ƒæ•°æ®æ¨¡å‹è§†å›¾é€šå¸¸æ˜¯æ—¶é—´ç›¸å…³çš„ï¼ŒåŒ…å«ç”¨äºç­›é€‰ç‰¹å®šæ—¶é—´èŒƒå›´å†…æ•°æ®è®°å½•çš„å­—æ®µã€‚ä¸ºäº†è¯„ä¼°äººå·¥æ™ºèƒ½æ¨¡å‹çš„æ€§èƒ½ï¼Œæ¨¡å‹è¡°é€€ç»„ä»¶åˆ©ç”¨æ¨¡å‹è®­ç»ƒè§†å›¾æ¥è·å–è‡ªä¸Šæ¬¡è®­ç»ƒä¼šè¯ä»¥æ¥äº§ç”Ÿçš„æ–°æ•°æ®ã€‚ç”±äºéšå¼åé¦ˆå·²è¢«æ•´åˆåˆ°åº”ç”¨æ•°æ®ä¸­ï¼Œè¿™ä¸ªå¯¹ç…§æ•°æ®é›†åŒ…å«äº†ç›®æ ‡å˜é‡çš„å®é™…å€¼ä»¥åŠè¾“å…¥å‚æ•°çš„å€¼ã€‚ç³»ç»Ÿä½¿ç”¨å½“å‰ç‰ˆæœ¬çš„å·²è®­ç»ƒæ¨¡å‹ï¼Œæ ¹æ®è¾“å…¥å€¼è®¡ç®—é¢„æµ‹ç»“æœã€‚è¿™æ˜¯æŒ‡æ ‡ç»„ä»¶çš„èŒè´£ï¼Œè¯¥ç»„ä»¶è¿˜è´Ÿè´£æ¯”è¾ƒå®é™…å€¼å’Œé¢„æµ‹å€¼ï¼Œä»¥è®¡ç®—é™æ€å…³é”®ç»©æ•ˆæŒ‡æ ‡ï¼ˆKPIï¼‰ã€‚æ¨¡å‹è¡°é€€ç»„ä»¶ä¼šè§£è¯»è¿™äº›KPIï¼Œå¹¶å»ºè®®ç›¸åº”çš„è¡ŒåŠ¨ï¼ˆä¾‹å¦‚é‡æ–°è®­ç»ƒæˆ–æ¨¡å‹é‡æ„ï¼‰ï¼Œè¿™äº›å»ºè®®ä¼šå‘ˆç°åœ¨ç›‘æ§è§†å›¾ä¸­ã€‚

---

13.3 Conclusion

**ã€è¯‘æ–‡ã€‘** 13.3 ç»“è®º

---

In the course of time, the prediction power of artificial intelligence models decreases due to changes in the data environment. Determining this time point and triggering retraining is the objective of model degradation. It is essential to anticipate how data will evolve over time in all artificial intelligence projects. In fact, before we use a model, its accuracy is at its highest. Model degradation is a phenomenon that has been extensively researched in academics for the past 20 years, although industry best practices still frequently overlook it. Thus, it is important to regularly assess model performance on new data sets. To know when to take action, these performance traces should be periodically compared and shown. There are many KPIs available to evaluate model performance. Without a strategy for routinely evaluating performance measurements and initiating model retraining or rebuilding, we are able to detect performance loss but lack a system for addressing it. Explicit and implicit feedback must be also taken into account, which are, in the context of ERP systems, complex to resolve. In this section, we deducted the business requirements and the necessary technical implementation for model degradation regarding ERP software. We proposed, for example, feedback loops as an important technique for resolving model degradation. Implicit, explicit, and delayed feedback loops were depicted.

**ã€è¯‘æ–‡ã€‘** éšç€æ—¶é—´çš„æ¨ç§»ï¼Œç”±äºæ•°æ®ç¯å¢ƒçš„å˜åŒ–ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ä¼šé€æ¸ä¸‹é™ã€‚ç¡®å®šè¿™ä¸€æ—¶é—´ç‚¹å¹¶è§¦å‘é‡æ–°è®­ç»ƒæ˜¯æ¨¡å‹è¡°é€€ç®¡ç†çš„ç›®æ ‡ã€‚åœ¨æ‰€æœ‰äººå·¥æ™ºèƒ½é¡¹ç›®ä¸­ï¼Œé¢„åˆ¤æ•°æ®å°†å¦‚ä½•éšæ—¶é—´æ¼”å˜è‡³å…³é‡è¦ã€‚äº‹å®ä¸Šï¼Œæ¨¡å‹åœ¨æŠ•å…¥ä½¿ç”¨ä¹‹å‰ï¼Œå…¶å‡†ç¡®æ€§å¾€å¾€å¤„äºæœ€é«˜æ°´å¹³ã€‚æ¨¡å‹è¡°é€€ç°è±¡åœ¨å­¦æœ¯ç•Œå·²è¢«å¹¿æ³›ç ”ç©¶äº†20å¹´ï¼Œå°½ç®¡è¡Œä¸šæœ€ä½³å®è·µä»ç»å¸¸å¿½è§†å®ƒã€‚å› æ­¤ï¼Œå®šæœŸè¯„ä¼°æ¨¡å‹åœ¨æ–°æ•°æ®é›†ä¸Šçš„æ€§èƒ½éå¸¸é‡è¦ã€‚ä¸ºäº†çŸ¥é“ä½•æ—¶é‡‡å–è¡ŒåŠ¨ï¼Œåº”å®šæœŸæ¯”è¾ƒå¹¶å±•ç¤ºè¿™äº›æ€§èƒ½è½¨è¿¹ã€‚æœ‰è®¸å¤šKPIå¯ç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚å¦‚æœæ²¡æœ‰åˆ¶å®šå¸¸è§„è¯„ä¼°æ€§èƒ½æŒ‡æ ‡å¹¶å¯åŠ¨æ¨¡å‹é‡æ–°è®­ç»ƒæˆ–é‡å»ºçš„ç­–ç•¥ï¼Œæˆ‘ä»¬è™½ç„¶èƒ½æ£€æµ‹åˆ°æ€§èƒ½æŸå¤±ï¼Œå´ç¼ºä¹è§£å†³è¯¥é—®é¢˜çš„ä½“ç³»ã€‚è¿˜å¿…é¡»è€ƒè™‘æ˜¾å¼å’Œéšå¼åé¦ˆï¼Œè€Œåœ¨ERPç³»ç»Ÿçš„èƒŒæ™¯ä¸‹ï¼Œè¿™äº›åé¦ˆçš„å¤„ç†é€šå¸¸å¾ˆå¤æ‚ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¨å¯¼å‡ºäº†é’ˆå¯¹ERPè½¯ä»¶æ¨¡å‹è¡°é€€çš„ä¸šåŠ¡éœ€æ±‚å’Œå¿…è¦çš„æŠ€æœ¯å®ç°ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æå‡ºåé¦ˆå¾ªç¯æ˜¯è§£å†³æ¨¡å‹è¡°é€€çš„é‡è¦æŠ€æœ¯ï¼Œå¹¶æè¿°äº†éšå¼ã€æ˜¾å¼å’Œå»¶è¿Ÿåé¦ˆå¾ªç¯ã€‚

---

14 Explanation of Results

**ã€è¯‘æ–‡ã€‘** ç¬¬14ç«  ç»“æœè§£é‡Š

---

In this chapter, we specify the business requirements and propose the solution concept for explainability. To build trust between human and machine, itâ€™s important to explain the results provided by artificial intelligence models. Transparency and traceability of artificial intelligence models are also needed for statutory reasons. However, depending on the underlying artificial intelligence techniques, this can be very challenging, for example, neuronal networks are hard to explain. In the context of ERP systems, additionally, the explanation must be transferred into a business language. Thus, user interface designers must investigate long time, for each use case translates the statistical numbers into the end userâ€™s business domain.

**ã€è¯‘æ–‡ã€‘** åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ˜ç¡®ä¸šåŠ¡éœ€æ±‚å¹¶æå‡ºå…³äºå¯è§£é‡Šæ€§çš„è§£å†³æ–¹æ¡ˆæ¦‚å¿µã€‚ä¸ºäº†åœ¨äººä¸æœºå™¨ä¹‹é—´å»ºç«‹ä¿¡ä»»ï¼Œè§£é‡Šäººå·¥æ™ºèƒ½æ¨¡å‹æä¾›çš„ç»“æœè‡³å…³é‡è¦ã€‚å‡ºäºæ³•è§„ç›‘ç®¡çš„åŸå› ï¼Œäººå·¥æ™ºèƒ½æ¨¡å‹çš„é€æ˜åº¦å’Œå¯è¿½æº¯æ€§ä¹Ÿæ˜¯å¿…éœ€çš„ã€‚ç„¶è€Œï¼Œæ ¹æ®åº•å±‚äººå·¥æ™ºèƒ½æŠ€æœ¯çš„ä¸åŒï¼Œè¿™å¯èƒ½æå…·æŒ‘æˆ˜æ€§ï¼Œä¾‹å¦‚ç¥ç»ç½‘ç»œå°±å¾ˆéš¾è§£é‡Šã€‚æ­¤å¤–ï¼Œåœ¨ERPç³»ç»Ÿçš„èƒŒæ™¯ä¸‹ï¼Œè¿™ç§è§£é‡Šå¿…é¡»è½¬åŒ–ä¸ºä¸šåŠ¡è¯­è¨€ã€‚å› æ­¤ï¼Œç”¨æˆ·ç•Œé¢è®¾è®¡å¸ˆå¿…é¡»æŠ•å…¥å¤§é‡æ—¶é—´è¿›è¡Œç ”ç©¶ï¼Œé’ˆå¯¹æ¯ä¸ªç”¨ä¾‹å°†ç»Ÿè®¡æ•°æ®è½¬åŒ–ä¸ºæœ€ç»ˆç”¨æˆ·ç†Ÿæ‚‰çš„ä¸šåŠ¡é¢†åŸŸè¯­è¨€ã€‚

---

14.1 Problem Statement

**ã€è¯‘æ–‡ã€‘** 14.1 é—®é¢˜é™ˆè¿°

---

When an algorithmâ€™s underlying model is adequately explained, and the rationale behind its results is made clear, a foundation of trust is established between humans and machines. Assessing trust is a crucial factor for humans to act based on a prediction. This element becomes even more significant in the context of ERP business applications, where users are legally responsible for every decision they make. The concept of explainable artificial intelligence implies that the logic behind the suggestions of an intelligent system can be elucidated in a timely and contextual manner. This approach facilitates the building of trust and allows for the anticipation of legal requirements related to automated decision-making. The need for explanations from artificial intelligence should be considered under one or more of the following circumstances:

**ã€è¯‘æ–‡ã€‘** å½“ç®—æ³•çš„åº•å±‚æ¨¡å‹å¾—åˆ°å……åˆ†è§£é‡Šï¼Œä¸”å…¶ç»“æœèƒŒåçš„é€»è¾‘è¢«é˜æ˜æ—¶ï¼Œäººç±»ä¸æœºå™¨ä¹‹é—´å°±å»ºç«‹äº†ä¿¡ä»»åŸºç¡€ã€‚ä¿¡ä»»åº¦è¯„ä¼°æ˜¯äººç±»åŸºäºé¢„æµ‹é‡‡å–è¡ŒåŠ¨çš„å…³é”®å› ç´ ã€‚è¿™ä¸€è¦ç´ åœ¨ERPä¸šåŠ¡åº”ç”¨çš„èƒŒæ™¯ä¸‹å˜å¾—å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºç”¨æˆ·å¯¹å…¶åšå‡ºçš„æ¯ä¸€é¡¹å†³ç­–éƒ½è´Ÿæœ‰æ³•å¾‹è´£ä»»ã€‚å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰çš„æ¦‚å¿µæ„å‘³ç€ï¼Œæ™ºèƒ½ç³»ç»Ÿå»ºè®®èƒŒåçš„é€»è¾‘å¯ä»¥è¢«åŠæ—¶ä¸”ç»“åˆä¸Šä¸‹æ–‡åœ°é˜æ˜ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºå»ºç«‹ä¿¡ä»»ï¼Œå¹¶èƒ½æå‰æ»¡è¶³ä¸è‡ªåŠ¨åŒ–å†³ç­–ç›¸å…³çš„æ³•å¾‹è¦æ±‚ã€‚åœ¨ä»¥ä¸‹ä¸€ç§æˆ–å¤šç§æƒ…å†µä¸‹ï¼Œåº”è€ƒè™‘å¯¹äººå·¥æ™ºèƒ½ç»“æœè¿›è¡Œè§£é‡Šçš„éœ€æ±‚ï¼š

---

â€¢ Criticality

**ã€è¯‘æ–‡ã€‘** â€¢ å…³é”®æ€§

---

When there is a substantial risk linked to making an incorrect decision and the actions taken are difficult to undo, an explanation for the systemâ€™s suggestion shall be necessary. Conversely, if the risk is minimal and actions can be easily reversed, users may not require an explanation.

**ã€è¯‘æ–‡ã€‘** å½“åšå‡ºé”™è¯¯å†³ç­–å­˜åœ¨é‡å¤§é£é™©ä¸”é‡‡å–çš„è¡ŒåŠ¨éš¾ä»¥æ’¤é”€æ—¶ï¼Œå¿…é¡»å¯¹ç³»ç»Ÿçš„å»ºè®®è¿›è¡Œè§£é‡Šã€‚ç›¸åï¼Œå¦‚æœé£é™©æå°ä¸”è¡ŒåŠ¨å¯ä»¥è½»æ¾é€†è½¬ï¼Œç”¨æˆ·å¯èƒ½ä¸éœ€è¦è§£é‡Šã€‚

---

â€¢ Complexity

**ã€è¯‘æ–‡ã€‘** â€¢ å¤æ‚æ€§

---

When users find it challenging to immediately evaluate the impact and quality of their decisions, they may need additional input. On the other hand, if users can readily determine whether a suggestion is suitable without any training, they may not need further information.

**ã€è¯‘æ–‡ã€‘** å½“ç”¨æˆ·è§‰å¾—éš¾ä»¥ç«‹å³è¯„ä¼°å…¶å†³ç­–çš„å½±å“å’Œè´¨é‡æ—¶ï¼Œä»–ä»¬å¯èƒ½éœ€è¦é¢å¤–çš„è¾“å…¥ä¿¡æ¯ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœç”¨æˆ·æ— éœ€ä»»ä½•åŸ¹è®­å°±èƒ½è½»æ˜“åˆ¤æ–­å»ºè®®æ˜¯å¦åˆé€‚ï¼Œä»–ä»¬å¯èƒ½ä¸éœ€è¦è¿›ä¸€æ­¥çš„ä¿¡æ¯ã€‚

---

â€¢ Transparency

**ã€è¯‘æ–‡ã€‘** â€¢ é€æ˜åº¦

---

When a business process is subject to rigorous auditing requirements, auditors must be able to trace transactions back and understand the reasoning behind each step of execution. However, if there are no auditing requirements, explanations may not be necessary, assuming they are also not required by end users.

**ã€è¯‘æ–‡ã€‘** å½“ä¸šåŠ¡æµç¨‹å—åˆ°ä¸¥æ ¼çš„å®¡è®¡è¦æ±‚çº¦æŸæ—¶ï¼Œå®¡è®¡å‘˜å¿…é¡»èƒ½å¤Ÿè¿½æº¯äº¤æ˜“å¹¶ç†è§£æ‰§è¡Œæ¯ä¸€æ­¥éª¤èƒŒåçš„æ¨ç†ã€‚ç„¶è€Œï¼Œå¦‚æœæ²¡æœ‰å®¡è®¡è¦æ±‚ï¼Œä¸”æœ€ç»ˆç”¨æˆ·ä¹Ÿä¸éœ€è¦ï¼Œé‚£ä¹ˆè§£é‡Šå¯èƒ½å°±ä¸æ˜¯å¿…é¡»çš„ã€‚

---

â€¢ Volatility

**ã€è¯‘æ–‡ã€‘** â€¢ æ³¢åŠ¨æ€§

---

When the artificial intelligence application needs to adjust to shifting conditions or requirements, it relies on continuous feedback. Conversely, if the feedback has little or no impact on the algorithmâ€™s output or the user experience, providing an additional explanation may be more of a distraction than a help.

**ã€è¯‘æ–‡ã€‘** å½“äººå·¥æ™ºèƒ½åº”ç”¨éœ€è¦é€‚åº”ä¸æ–­å˜åŒ–çš„æ¡ä»¶æˆ–éœ€æ±‚æ—¶ï¼Œå®ƒä¾èµ–äºæŒç»­çš„åé¦ˆã€‚ç›¸åï¼Œå¦‚æœåé¦ˆå¯¹ç®—æ³•çš„è¾“å‡ºæˆ–ç”¨æˆ·ä½“éªŒå½±å“ç”šå¾®æˆ–æ²¡æœ‰å½±å“ï¼Œæä¾›é¢å¤–çš„è§£é‡Šå¯èƒ½ä¸å…¶è¯´æ˜¯å¸®åŠ©ï¼Œä¸å¦‚è¯´æ˜¯ä¸€ç§å¹²æ‰°ã€‚

---

14.2 Solution Proposal

**ã€è¯‘æ–‡ã€‘** 14.2 è§£å†³æ–¹æ¡ˆå»ºè®®

---

When considering ERP software, we are faced with two primary inquiries related to explainable artificial intelligence:
â€¢ What design principles should we use to incorporate explainable artificial intelligence into the user interfaces?
â€¢ What elements need to be incorporated into the ERP backend to facilitate explainable artificial intelligence?

**ã€è¯‘æ–‡ã€‘** åœ¨è€ƒè™‘ERPè½¯ä»¶æ—¶ï¼Œæˆ‘ä»¬é¢ä¸´ä¸¤ä¸ªä¸å¯è§£é‡Šäººå·¥æ™ºèƒ½ç›¸å…³çš„ä¸»è¦é—®é¢˜ï¼š
â€¢ æˆ‘ä»¬åº”è¯¥ä½¿ç”¨ä»€ä¹ˆè®¾è®¡åŸåˆ™å°†å¯è§£é‡Šäººå·¥æ™ºèƒ½èå…¥ç”¨æˆ·ç•Œé¢ï¼Ÿ
â€¢ éœ€è¦åœ¨ERPåç«¯é›†æˆå“ªäº›è¦ç´ ä»¥ä¿ƒè¿›å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼Ÿ

---

This chapter will not delve into the techniques used to generate explanations for specific artificial intelligence algorithms. These methods are well-known in the data science community and taken as granted. Therefore, this discussion will focus solely on aspects specific to ERP, as guided by the aforementioned questions.

**ã€è¯‘æ–‡ã€‘** æœ¬ç« ä¸ä¼šæ·±å…¥æ¢è®¨ç”¨äºä¸ºç‰¹å®šäººå·¥æ™ºèƒ½ç®—æ³•ç”Ÿæˆè§£é‡Šçš„æŠ€æœ¯ã€‚è¿™äº›æ–¹æ³•åœ¨æ•°æ®ç§‘å­¦ç•Œä¼—æ‰€å‘¨çŸ¥å¹¶è¢«è§†ä¸ºé€šç”¨æ ‡å‡†ã€‚å› æ­¤ï¼Œæ ¹æ®ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ¬¡è®¨è®ºå°†ä»…å…³æ³¨ERPç‰¹æœ‰çš„æ–¹é¢ã€‚

---

14.2.1 User Interface

**ã€è¯‘æ–‡ã€‘** 14.2.1 ç”¨æˆ·ç•Œé¢

---

The level of information that individuals require to comprehend a system proposal can fluctuate. This variation is influenced by the specific technique of artificial intelligence being used, the context in which it is used, the task the user is performing, and the role of the user. We can distinguish between three levels of explanation:

**ã€è¯‘æ–‡ã€‘** ä¸ªäººç†è§£ç³»ç»Ÿå»ºè®®æ‰€éœ€çš„ä¿¡æ¯è¯¦ç»†ç¨‹åº¦å¯èƒ½ä¼šæœ‰æ‰€æ³¢åŠ¨ã€‚è¿™ç§å·®å¼‚å—æ‰€ä½¿ç”¨çš„ç‰¹å®šäººå·¥æ™ºèƒ½æŠ€æœ¯ã€ä½¿ç”¨èƒŒæ™¯ã€ç”¨æˆ·æ‰§è¡Œçš„ä»»åŠ¡ä»¥åŠç”¨æˆ·è§’è‰²çš„å½±å“ã€‚æˆ‘ä»¬å¯ä»¥åŒºåˆ†ä¸‰ä¸ªè§£é‡Šå±‚çº§ï¼š

---

1. Indicator (What?)

**ã€è¯‘æ–‡ã€‘** 1. æŒ‡ç¤ºå™¨ï¼ˆæ˜¯ä»€ä¹ˆï¼Ÿï¼‰

---

This represents the most basic level of explanation. An indicator becomes necessary whenever there is an output from the artificial intelligence system. This indicator also serves as the entry point to the subsequent level of explanation.

**ã€è¯‘æ–‡ã€‘** è¿™æ˜¯æœ€åŸºç¡€çš„è§£é‡Šå±‚çº§ã€‚åªè¦äººå·¥æ™ºèƒ½ç³»ç»Ÿæœ‰è¾“å‡ºï¼ŒæŒ‡ç¤ºå™¨å°±æ˜¯å¿…è¦çš„ã€‚è¯¥æŒ‡ç¤ºå™¨ä¹Ÿä½œä¸ºé€šå¾€ä¸‹ä¸€çº§è§£é‡Šçš„å…¥å£ç‚¹ã€‚

---

