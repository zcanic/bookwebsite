# Embedding Artificial Intelligence into ERP Software A Conceptual View on Business AI with Examples from SAP S4HANA (Siar Sarferaz) (z-library.sk, 1lib.sk, z-lib.sk) - ç¬¬78éƒ¨åˆ†

**åŸå§‹é¡µç **: 232 - 234
**æ®µè½æ•°é‡**: 15
**ç¿»è¯‘å·¥å…·**: Claude Code CLI

## ğŸ“– ç¿»è¯‘ç­–ç•¥å’Œä¸Šä¸‹æ–‡ä¿¡æ¯

- **ä½¿ç”¨çš„ç¿»è¯‘ç­–ç•¥**: å®Œæ•´ä¸Šä¸‹æ–‡
- **å‰æ–‡ä¸Šä¸‹æ–‡é¡µç **: 231-231 (792 å­—ç¬¦)
- **åæ–‡ä¸Šä¸‹æ–‡é¡µç **: 235-235 (63 å­—ç¬¦)

*æ³¨ï¼šç¿»è¯‘æ—¶å·²ä½¿ç”¨"å®Œæ•´ä¸Šä¸‹æ–‡"ç­–ç•¥ï¼Œå‚è€ƒå‰åæ–‡ä¸Šä¸‹æ–‡ä»¥ç¡®ä¿æœ¯è¯­ä¸€è‡´æ€§å’Œå†…å®¹è¿è´¯æ€§*

**æœ¯è¯­è¯´æ˜**: 1. 'Intrinsic' è¯‘ä¸º 'å†…åœ¨'ï¼Œ'Post hoc' è¯‘ä¸º 'äº‹å'ï¼Œè¿™æ˜¯æœºå™¨å­¦ä¹ å¯è§£é‡Šæ€§ï¼ˆExplainable AIï¼‰é¢†åŸŸçš„é€šç”¨æœ¯è¯­ã€‚ 2. 'End-of-period closing' è¯‘ä¸º 'æœŸæœ«ç»“è´¦'ï¼Œç¬¦åˆERPè´¢åŠ¡æ¨¡å—çš„ä¸“ä¸šè¯­å¢ƒã€‚ 3. ç¬¬ä¸€æ®µå¼€å¤´ç»“åˆäº†å‰æ–‡ä¸Šä¸‹æ–‡è¡¥å…¨äº†è¯­ä¹‰ã€‚

---

of the artificial intelligence feature. The credibility of an explanation is contingent on its life cycle. We distinguish between two scenarios based on the lifespan of the explanation:

**ã€è¯‘æ–‡ã€‘** ä¸å¯ä¿¡çš„ç»“æœä¼šä¾µèš€ç”¨æˆ·çš„ä¿¡ä»»ï¼Œå¹¶é˜»ç¢äººå·¥æ™ºèƒ½åŠŸèƒ½çš„é‡‡ç”¨ã€‚è§£é‡Šçš„å¯ä¿¡åº¦å–å†³äºå…¶ç”Ÿå‘½å‘¨æœŸã€‚æ ¹æ®è§£é‡Šçš„å­˜ç»­æ—¶é—´ï¼Œæˆ‘ä»¬åŒºåˆ†äº†ä»¥ä¸‹ä¸¤ç§åœºæ™¯ï¼š

---

â€¢ Static explanations are appropriate for offline learning where the system does not change the approximation of the target function once the initial training phase is finished. We presume that the requirement for a repeated explanation of the model may reduce or even vanish over time as the user becomes more familiar with the system. Once the user comprehends the main principle of the underlying service (algorithm), it should be considered to omit explanations at the model level. Moreover, the explanation should always be presented again if the artificial intelligence model is changed or updated. This is applicable to explanations that are hard-coded or manually maintained.

**ã€è¯‘æ–‡ã€‘** â€¢ **é™æ€è§£é‡Š**é€‚ç”¨äºç¦»çº¿å­¦ä¹ ï¼ˆoffline learningï¼‰ï¼Œå³ä¸€æ—¦åˆå§‹è®­ç»ƒé˜¶æ®µå®Œæˆï¼Œç³»ç»Ÿå°±ä¸å†æ”¹å˜å¯¹ç›®æ ‡å‡½æ•°çš„æ‹Ÿåˆã€‚æˆ‘ä»¬å‡è®¾éšç€ç”¨æˆ·å¯¹ç³»ç»Ÿè¶Šæ¥è¶Šç†Ÿæ‚‰ï¼Œå¯¹æ¨¡å‹é‡å¤è§£é‡Šçš„éœ€æ±‚å¯èƒ½ä¼šå‡å°‘ç”šè‡³æ¶ˆå¤±ã€‚ä¸€æ—¦ç”¨æˆ·ç†è§£äº†åº•å±‚æœåŠ¡ï¼ˆç®—æ³•ï¼‰çš„ä¸»è¦åŸç†ï¼Œåº”è€ƒè™‘çœç•¥æ¨¡å‹å±‚é¢çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œå¦‚æœäººå·¥æ™ºèƒ½æ¨¡å‹å‘ç”Ÿå˜æ›´æˆ–æ›´æ–°ï¼Œåˆ™å¿…é¡»å†æ¬¡å±•ç¤ºè§£é‡Šã€‚è¿™é€‚ç”¨äºé‚£äº›ç¡¬ç¼–ç æˆ–æ‰‹åŠ¨ç»´æŠ¤çš„è§£é‡Šã€‚

---

â€¢ Dynamic explanations are necessary for online learning services powered by artificial intelligence where data is sequentially made available and is used to update the model at each step. This kind of explanation is produced every time the artificial intelligence model has learned and adjusted its model. In most instances, itâ€™s not required to completely change the appearance and content of the explanations. However, if a service powered by artificial intelligence discovers new rules for processing items in the system, this newly gained knowledge must be reflected in its explanations. We should adapt the primary artificial intelligence explanation to mirror newly implemented processing principles and alter the list of decision criteria whenever objectives are modified.

**ã€è¯‘æ–‡ã€‘** â€¢ **åŠ¨æ€è§£é‡Š**å¯¹äºåŸºäºäººå·¥æ™ºèƒ½çš„åœ¨çº¿å­¦ä¹ ï¼ˆonline learningï¼‰æœåŠ¡æ˜¯å¿…è¦çš„ï¼Œåœ¨è¿™ç±»æœåŠ¡ä¸­ï¼Œæ•°æ®æ˜¯é™†ç»­è·å–çš„ï¼Œå¹¶ç”¨äºåœ¨æ¯ä¸€æ­¥æ›´æ–°æ¨¡å‹ã€‚æ¯å½“äººå·¥æ™ºèƒ½æ¨¡å‹è¿›è¡Œäº†å­¦ä¹ å¹¶è°ƒæ•´äº†æ¨¡å‹æ—¶ï¼Œå°±ä¼šç”Ÿæˆè¿™ç§è§£é‡Šã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä¸éœ€è¦å®Œå…¨æ”¹å˜è§£é‡Šçš„å¤–è§‚å’Œå†…å®¹ã€‚ç„¶è€Œï¼Œå¦‚æœäººå·¥æ™ºèƒ½æœåŠ¡å‘ç°äº†å¤„ç†ç³»ç»Ÿå†…é¡¹ç›®çš„æ–°è§„åˆ™ï¼Œè¿™ç§æ–°è·å¾—çš„çŸ¥è¯†å¿…é¡»åæ˜ åœ¨å…¶è§£é‡Šä¸­ã€‚æˆ‘ä»¬åº”å½“è°ƒæ•´ä¸»è¦çš„äººå·¥æ™ºèƒ½è§£é‡Šä»¥åæ˜ æ–°å®æ–½çš„å¤„ç†åŸåˆ™ï¼Œå¹¶åœ¨ç›®æ ‡ä¿®æ”¹æ—¶æ›´æ”¹å†³ç­–æ ‡å‡†åˆ—è¡¨ã€‚

---

Before we commence with the design of the artificial intelligence application, we should conduct research to determine the answers to the following questions:

**ã€è¯‘æ–‡ã€‘** åœ¨å¼€å§‹è®¾è®¡äººå·¥æ™ºèƒ½åº”ç”¨ä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥è¿›è¡Œè°ƒç ”ä»¥ç¡®å®šä»¥ä¸‹é—®é¢˜çš„ç­”æ¡ˆï¼š

---

â€¢ Is the user anticipating an explanation? When the potential risks linked to a task are relatively minor and the outcomes can be effortlessly rolled back, users typically donâ€™t seek an explanation for the systemâ€™s suggestion.

**ã€è¯‘æ–‡ã€‘** â€¢ **ç”¨æˆ·æ˜¯å¦æœŸå¾…è§£é‡Šï¼Ÿ** å½“ä¸ä»»åŠ¡ç›¸å…³çš„æ½œåœ¨é£é™©ç›¸å¯¹è¾ƒå°ï¼Œä¸”ç»“æœå¯ä»¥è½»æ¾å›æ»šæ—¶ï¼Œç”¨æˆ·é€šå¸¸ä¸ä¼šå¯»æ±‚é’ˆå¯¹ç³»ç»Ÿå»ºè®®çš„è§£é‡Šã€‚

---

â€¢ What degree of automation are we striving for? The level of automation can significantly alter the use case, the roles of the target users, and the applicationâ€™s capabilities.

**ã€è¯‘æ–‡ã€‘** â€¢ **æˆ‘ä»¬è¿½æ±‚çš„è‡ªåŠ¨åŒ–ç¨‹åº¦æ˜¯å¤šå°‘ï¼Ÿ** è‡ªåŠ¨åŒ–æ°´å¹³ä¼šæ˜¾è‘—æ”¹å˜ç”¨ä¾‹ã€ç›®æ ‡ç”¨æˆ·çš„è§’è‰²ä»¥åŠåº”ç”¨çš„èƒ½åŠ›ã€‚

---

â€¢ Besides the intended business user, what other roles participate in the experience? The interaction with artificial intelligence systems also includes technical roles that are not business-related. We should also take into account roles that contribute to development, support, or maintenance.

**ã€è¯‘æ–‡ã€‘** â€¢ **é™¤äº†é¢„æœŸçš„ä¸šåŠ¡ç”¨æˆ·ï¼Œè¿˜æœ‰å“ªäº›è§’è‰²å‚ä¸ä½“éªŒï¼Ÿ** ä¸äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„äº¤äº’ä¹ŸåŒ…æ‹¬éä¸šåŠ¡ç›¸å…³çš„æŠ€æœ¯è§’è‰²ã€‚æˆ‘ä»¬è¿˜åº”è€ƒè™‘åˆ°é‚£äº›è´Ÿè´£å¼€å‘ã€æ”¯æŒæˆ–ç»´æŠ¤çš„è§’è‰²ã€‚

---

â€¢ How transparent or trackable does the artificial intelligence service needs to be? Some use cases are more sensitive than others. Enterprise applications, for instance, often face stricter auditing requirements due to legal and regulatory mandates, compared to consumer software.

**ã€è¯‘æ–‡ã€‘** â€¢ **äººå·¥æ™ºèƒ½æœåŠ¡éœ€è¦å¤šé«˜çš„é€æ˜åº¦æˆ–å¯è¿½è¸ªæ€§ï¼Ÿ** æŸäº›ç”¨ä¾‹æ¯”å…¶ä»–ç”¨ä¾‹æ›´æ•æ„Ÿã€‚ä¾‹å¦‚ï¼Œä¸æ¶ˆè´¹è€…è½¯ä»¶ç›¸æ¯”ï¼Œç”±äºæ³•å¾‹å’Œç›‘ç®¡çš„å¼ºåˆ¶è§„å®šï¼Œä¼ä¸šçº§åº”ç”¨é€šå¸¸é¢ä¸´æ›´ä¸¥æ ¼çš„å®¡è®¡è¦æ±‚ã€‚

---

â€¢ Can users generally comprehend the displayed information and data, and can they deduce the subsequent actions and their effects? While artificial intelligence can aid in enhancing even the simplest tasks, it may be overengineered to offer artificial intelligence explanations when the user already comprehends the outcomes and their implications.

**ã€è¯‘æ–‡ã€‘** â€¢ **ç”¨æˆ·é€šå¸¸èƒ½ç†è§£æ˜¾ç¤ºçš„ä¿¡æ¯å’Œæ•°æ®ï¼Œå¹¶æ¨æ–­å‡ºåç»­è¡ŒåŠ¨åŠå…¶å½±å“å—ï¼Ÿ** è™½ç„¶äººå·¥æ™ºèƒ½ç”šè‡³å¯ä»¥å¸®åŠ©å¢å¼ºæœ€ç®€å•çš„ä»»åŠ¡ï¼Œä½†å¦‚æœç”¨æˆ·å·²ç»ç†è§£äº†ç»“æœåŠå…¶å«ä¹‰ï¼Œå†æä¾›äººå·¥æ™ºèƒ½è§£é‡Šå¯èƒ½å°±å±äºè¿‡åº¦è®¾è®¡ï¼ˆoverengineeringï¼‰äº†ã€‚

---

â€¢ What is the potential damage to the customerâ€™s business if data is processed incorrectly? Some tasks in processes are inherently critical and can lead to severe consequences. Artificial intelligence explanations must protect against such scenarios and assist in preventing any harm or disruption.

**ã€è¯‘æ–‡ã€‘** â€¢ **å¦‚æœæ•°æ®å¤„ç†ä¸æ­£ç¡®ï¼Œå¯¹å®¢æˆ·ä¸šåŠ¡çš„æ½œåœ¨æŸå®³æ˜¯ä»€ä¹ˆï¼Ÿ** æµç¨‹ä¸­çš„æŸäº›ä»»åŠ¡æœ¬è´¨ä¸Šæ˜¯å…³é”®æ€§çš„ï¼Œå¯èƒ½å¯¼è‡´ä¸¥é‡åæœã€‚äººå·¥æ™ºèƒ½è§£é‡Šå¿…é¡»é˜²èŒƒæ­¤ç±»åœºæ™¯ï¼Œå¹¶ååŠ©é˜²æ­¢ä»»ä½•æŸå®³æˆ–ä¸­æ–­ã€‚

---

â€¢ How simple or complex is it to undo changes made to the system or process? In situations where the user is up against deadlines, end-of-period closing, or other tasks that must be successful on the first attempt, providing supportive information to the user is vital. However, explanations may not be necessary if itâ€™s feasible to instantly reverse everything in the event of a failure.

**ã€è¯‘æ–‡ã€‘** â€¢ **æ’¤é”€å¯¹ç³»ç»Ÿæˆ–æµç¨‹æ‰€åšçš„æ›´æ”¹æœ‰å¤šç®€å•æˆ–å¤æ‚ï¼Ÿ** åœ¨ç”¨æˆ·é¢ä¸´æˆªæ­¢æ—¥æœŸã€æœŸæœ«ç»“è´¦æˆ–å…¶ä»–å¿…é¡»ä¸€æ¬¡æˆåŠŸçš„ä»»åŠ¡æ—¶ï¼Œå‘ç”¨æˆ·æä¾›æ”¯æŒæ€§ä¿¡æ¯è‡³å…³é‡è¦ã€‚ç„¶è€Œï¼Œå¦‚æœåœ¨å¤±è´¥å‘ç”Ÿæ—¶å¯ä»¥ç«‹å³é€†è½¬ä¸€åˆ‡ï¼Œé‚£ä¹ˆè§£é‡Šå¯èƒ½å°±ä¸å¿…è¦äº†ã€‚

---

Does the business case necessitate constant adaptation? We presume that as the user becomes more experienced, the need for a repeated (static) explanation of the model may decrease or even vanish. However, if the algorithm powered by artificial intelligence learns dynamically, users must always be kept informed of changing conditions (dynamic explanations).

**ã€è¯‘æ–‡ã€‘** ä¸šåŠ¡åœºæ™¯æ˜¯å¦éœ€è¦ä¸æ–­çš„é€‚åº”ï¼Ÿæˆ‘ä»¬å‡è®¾éšç€ç”¨æˆ·ç»éªŒçš„ä¸°å¯Œï¼Œå¯¹æ¨¡å‹è¿›è¡Œé‡å¤ï¼ˆé™æ€ï¼‰è§£é‡Šçš„éœ€æ±‚å¯èƒ½ä¼šå‡å°‘ç”šè‡³æ¶ˆå¤±ã€‚ä½†æ˜¯ï¼Œå¦‚æœç”±äººå·¥æ™ºèƒ½é©±åŠ¨çš„ç®—æ³•æ˜¯åŠ¨æ€å­¦ä¹ çš„ï¼Œåˆ™å¿…é¡»å§‹ç»ˆè®©ç”¨æˆ·çŸ¥æ™“ä¸æ–­å˜åŒ–çš„æ¡ä»¶ï¼ˆåŠ¨æ€è§£é‡Šï¼‰ã€‚

---

14.2.2 Backend Processes

**ã€è¯‘æ–‡ã€‘** 14.2.2 åç«¯æµç¨‹

---

The concept of explainability can be realized by either limiting the intricacy of the artificial intelligence model, which is an intrinsic approach, or by implementing techniques that scrutinize the model after its training phase, known as the post hoc method. Intrinsic interpretability is associated with artificial intelligence models that are deemed explainable due to their uncomplicated structure, such as concise decision trees or sparse linear models. Itâ€™s worth noting that there are established explanation methods for artificial intelligence algorithms that are widely recognized within the data scientist community, but these are not the primary focus of this discussion. In the context of business applications, itâ€™s crucial to select methods that elucidate both the individual prediction and the overall behavior of the model.

**ã€è¯‘æ–‡ã€‘** å¯è§£é‡Šæ€§çš„æ¦‚å¿µå¯ä»¥é€šè¿‡ä¸¤ç§æ–¹å¼å®ç°ï¼šä¸€æ˜¯é™åˆ¶äººå·¥æ™ºèƒ½æ¨¡å‹çš„å¤æ‚æ€§ï¼Œè¿™æ˜¯ä¸€ç§**å†…åœ¨ï¼ˆintrinsicï¼‰**æ–¹æ³•ï¼›äºŒæ˜¯å®æ–½åœ¨è®­ç»ƒé˜¶æ®µç»“æŸåå®¡æŸ¥æ¨¡å‹çš„æŠ€æœ¯ï¼Œå³æ‰€è°“çš„**â€œäº‹åâ€ï¼ˆpost hocï¼‰**æ–¹æ³•ã€‚å†…åœ¨å¯è§£é‡Šæ€§é€šå¸¸ä¸é‚£äº›å› ç»“æ„ç®€å•è€Œè¢«è®¤ä¸ºå¯è§£é‡Šçš„äººå·¥æ™ºèƒ½æ¨¡å‹ç›¸å…³ï¼Œä¾‹å¦‚ç®€æ´çš„å†³ç­–æ ‘æˆ–ç¨€ç–çº¿æ€§æ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ•°æ®ç§‘å­¦å®¶ç¤¾åŒºä¸­ï¼Œæœ‰ä¸€äº›å¹¿æ³›è®¤å¯çš„æˆç†Ÿäººå·¥æ™ºèƒ½ç®—æ³•è§£é‡Šæ–¹æ³•ï¼Œä½†è¿™äº›ä¸æ˜¯æœ¬æ¬¡è®¨è®ºçš„é‡ç‚¹ã€‚åœ¨å•†ä¸šåº”ç”¨çš„èƒŒæ™¯ä¸‹ï¼Œè‡³å…³é‡è¦çš„æ˜¯é€‰æ‹©æ—¢èƒ½é˜æ˜å•ä¸ªé¢„æµ‹åˆèƒ½é˜æ˜æ¨¡å‹æ•´ä½“è¡Œä¸ºçš„æ–¹æ³•ã€‚

---

14.5
The explanations that are both global and local in nature often take the form of statistical metrics. These metrics need to be converted into the specific language of the end userâ€™s domain. The explanation component aids in this process technically through its ability to map these metrics accordingly. This presents a significant challenge for those designing the user interface.

**ã€è¯‘æ–‡ã€‘** 14.5
å…¼å…·å…¨å±€å’Œå±€éƒ¨æ€§è´¨çš„è§£é‡Šé€šå¸¸é‡‡å–ç»Ÿè®¡æŒ‡æ ‡çš„å½¢å¼ã€‚è¿™äº›æŒ‡æ ‡éœ€è¦è¢«è½¬æ¢ä¸ºæœ€ç»ˆç”¨æˆ·é¢†åŸŸçš„ç‰¹å®šè¯­è¨€ã€‚è§£é‡Šç»„ä»¶é€šè¿‡å…¶ç›¸åº”çš„æŒ‡æ ‡æ˜ å°„èƒ½åŠ›ï¼Œåœ¨æŠ€æœ¯å±‚é¢è¾…åŠ©è¿™ä¸€è¿‡ç¨‹ã€‚è¿™å¯¹ç”¨æˆ·ç•Œé¢çš„è®¾è®¡è€…æ¥è¯´æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚

---

